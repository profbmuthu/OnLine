---
<!-- .slide: data-autoslide="10000" -->
### Hadoop Administration
<br>
<span style="color:#e49436">Play Hadoop</span>
<br>
<span style="color:gray">-</span>
<br>
<span style="color:#e49436">Dr.B.Muthukumaran</span>
---

<!-- .slide: data-autoslide="2000" -->

## Agenda <span style="color: #e49436">Sessions</span>
### <span class="fragment" data-fragment-index="1" data-autoslide="2000"> Hadoop Management <span style="color: #666666">.</span>
<br>
### <span class="fragment" data-fragment-index="2" data-autoslide="3500">Administration<span style="color: #e49436">Management</span>. and <span style="color: #e49436">Guidelines</span>.</li>

---
<!-- .slide: data-autoslide="2000" -->

### Agenda
- Network
  + Vulnerability analysis
- Email Forensics 
  + Email Forensics - email spoofing – 
  + Phishing – mail header analysiw

---
<!-- .slide: data-autoslide="2000" -->
### Agenda
- Malware Concetps 
  + Virus - Components - Function of replicator, 
  + concealer and dispatcher- Trigger Mechanisms- Virus families  
  + worms - Types - Families - sandboxing
  + Trojans and Backdoors, Types of Trojans - 

---
<!-- .slide: data-autoslide="2000" -->

### Agenda
- Botnets
  + Botnets - types of botnet- Structure of bots – 
  + Crime bots - Spamming bots - 
  + DoS – DDoS Attacks – types - Honey Pots

---
<!-- .slide: data-autoslide="2000" -->

### Agenda
- Network forensics  
  + Key Loggers - Port Scans – SYN flood - 
  + Protocols Susceptible to Sniffing - Active and Passive Sniffing- 
  + Wireshark – Capture and Display Filters - pcap analysis – Problems -  
  + Forensic evidences - log analysis & evidence collection. 
  ---
<!-- .slide: data-autoslide="11000" -->
### <span style="color: #e49436">What is data?</span>
- Data is information 
- It is  translated into a form that is more convenient to move or process. 
- From a business context, data is information converted into binary digital form.  
- Data comes in different shapes, sizes, and granularity. 
- It also comes with a lot of uncertainty attached.
---
<!-- .slide: data-autoslide="11000" -->
### <span style="color: #e49436">What is dataset?</span>
- Data is usually collected as data points. 
- Means totals, averages, and medians are only a small part of what a data point is about.  
- Connection between data and what it represents is key to visualization. 
- Data is the key to data analysis and deeper understanding.
---
<!-- .slide: data-autoslide="11000" -->
### <span style="color: #e49436">What is dataset?</span>
- Data set is a snapshot of data in time 
- It captures something that moves and changes. 
- It refers to data selected and arranged in rows and columns. 
- Column represents a particular variable.  
- Row corresponds to a given member of the data set in question. Each value is known as a datum. 
---
<!-- .slide: data-autoslide="11000" -->
### <span style="color: #e49436">What is dataset?</span>
- Data set may comprise data for one or more members, corresponding to the number of rows 
- Data set collection of data points forming aggregates and statistical summaries. 
- Data set generates your  means, medians, and standard deviations.
---
<!-- .slide: data-autoslide="11000" -->
### <span style="color: #e49436">Data Generation?</span>
- Data is being generated faster and faster as more and more people take to the Web. 
- Customers generate data of all kinds, 
- Generated data is difficult to control.  
- Information is now widely available on external events such as market trends, industry news, and competitors’ movements. 
- Led to increasing interest in methods for extracting useful information and knowledge from data—the realm of data science.
---
### What is a <span style="color: #e49436">Types of Data</span>? 
- Data regardless of type, location, and source increasingly has become a core business asset for an enterprise 
- It is now categorized as belonging to two camps:
+ Internal data (enterprise application data) and
+ External data (e.g., web data)
---
<!-- .slide: data-autoslide="2000" -->
### What is a <span style="color: #e49436">Types of Data</span>? 
- There are two types of data in the real world:
+ Static data. (Collected Data). The volume of static data, for example national census data and human genomic data, will not change over time.
+ Dynamic data (Data being served continuously) Dynamic data, such as streaming log data and social network streaming data, the volume increases over time.
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> BigData.</span>
- Hadoop software are revolutionizing the Internet services community 
- Achieved by building scalable systems infrastructure for data intensive applications. 
- Business best practices” - increasingly towards basing decisions off data  rather than instinct and theory
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> BigData.</span>
- Corporate thirst for systems that can manage, process, and granularly analyze data is becoming insatiable. 
- Venture capitalists are very much aware of this trend, 
- Funded no fewer than a dozen new companies in recent years 
- e.g., Netezza, Vertica, DATAllegro, Greenplum, Aster Data, Infobright, Kickfire, Dataupia, ParAccel, and Exasol), 
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> BigData.</span>
- Big Data is data in large sizes 
- It goes beyond the ability of commonly used software tools to collect, manage, and process within a tolerable elapsed time. 
- Big data refers to datasets whose size is beyond the ability of typical database software tools
- Used  to capture, store, manage, and analyze data. 
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> BigData.</span>
- Big data essentially means datasets that are too large for traditional data processing systems.
- It is data that exceeds the processing capacity of conventional database defined systems. 
- It is too big, moves too fast, or doesn’t fit the strictures of conventional database architectures. 
- Valuable patterns and information, previously hidden because of the amount of work required to extract them. 
- It applies to information that can’t be processed or analyzed using traditional processes or tools. 
- The value of big data to an organization falls into two categories: analytical use and enabling new products
---
<!-- .slide: data-autoslide="2000" -->
### What is <span style="color: #e49436"> Characteristics of bigdata</span>
- Big data has become viable 
- Cost effective approaches have emerged to tame the volume, velocity, variety and variability of massive data. 
- Three Vs of volume, velocity, and variety are commonly used 
-  Helpful lens through which to view and understand the nature of the data 
- Helpful to understand software platforms available to exploit them. 
---
<!-- .slide: data-autoslide="2000" -->
### What is <span style="color: #e49436"> Data Volume</span>
- Amount of data being generated is increasing day by day. 
- Rate of increase of data is accelerating continuously. 
- Volume of data being made publicly available increases every year. 
- Organizations no longer have to merely manage their own data. 
- As data volume increases, the value of different data records will decrease in proportion to age, type, richness, and quantity among other factors. 

---
<!-- .slide: data-autoslide="2000" -->
### What is <span style="color: #e49436"> Data Volume</span>
- Data generated from emails to Facebook posts, from purchase histories to web links, is contributing to the growth of data. 
- Current challenge is in extracting value from this data.  
- Sometimes this means particular data elements, and at other times, the focus is instead on identifying trends and relationships between pieces of data. 
- Includes the current data volume

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Data Velocity.</span>
- Velocity of data is the measure of how fast the data is coming in.  
- Data volume measures the amount of data available to an organization.
- Data velocity measures the speed of data creation, streaming, and aggregation.
- eCommerce has rapidly increased the speed and richness of data used for different business transactions (for example, web-site clicks). 
- Data velocity management is much more than a bandwidth issue; it is also an ingest issue.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Data Velocity.</span>
- Earlier companies analyzed data using a batch process. 
- A chunk of data, is submitted as a job to the server. The job gets executed and results are delivered. 
- That scheme works when the incoming data rate is slower than the batch processing rate and when the result is useful despite the delay. 
- With the new sources of data such as social and mobile applications, the batch process breaks down. 
- The data is now streaming into the server in real time, in a continuous fashion and the result is only useful if the delay is very short
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Data Variety.</span>
- Data can help inform intelligent policy making 
- It provides innovative kindling for investigative journalism, 
- Even when data is freely available, it can be shared using data formats that are touch for consuption.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Data Variety.</span>

- Data variety is a measure of the richness of the data representation – text, images video, audio, etc. 

- From an analytic perspective, it is probably the biggest obstacle to effectively using large volumes of data. 

-Incompatible data formats, non-aligned data structures, and inconsistent data semantics represents significant challenges that can lead to analytic sprawl.

---
<!-- .slide: data-autoslide="2000" -->
### Types of<span style="color: #e49436"> Data Versatility.</span>
- Versatility refers to the adaptability of a system to analytical queries of varying complexity. 
- Versatility also refers to system flexibility in terms of query languages supported, and control in terms of data preparation, placement and management. 
- Within this data lie valuable patterns and information, previously hidden because of the amount of work required to extract them
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Bigdata Challenges</span>
- Increasingly, organizations today are facing more and more Big Data challenges. 
- The emergence of big data into the enterprise brings with it a necessary counterpart: agility.
- As data sizes grow, the concept of providing a single repository for all this data is not always the most practical solution.
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Benefits of bigdata</span>
- Big data analytics reveals insights hidden previously by data too costly to process, 

- It can be peer influence among customers, revealed by analyzing shoppers’ transactions and social and geographical data in real time. 

- Meaning of “real time” varies depending on the context in which it is used. 

- Real-time denotes the ability to process data as it arrives, 

- It does not permit storing the data and retrieving it at some point in the future 

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Benefits of bigdata</span>
-  For  an online merchant, “the present” means the attention span of a potential customer. 

-  Processing time of a transaction exceeds the customer’s attention span, the merchant doesn’t consider it real time.

-  For an options trader,  real time means milliseconds. For a guided missile, real time means microseconds. 

-  Real-time big data analytics is an iterative process involving multiple tools and systems.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Hadoop</span>
- Hadoop is based on the Google paper on MapReduce 

- Paper published in 2004, with development from  2005. 

- Hadoop was developed to support the open-source web search engine project called Nutch.

- Hadoop separated from Nutch and became its own project under the Apache Foundation. 

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Hadoop</span>
- Hadoop is the best–known MapReduce framework in the market. 
- Companies have grown around Hadoop to provide support, consulting, and training services for the Hadoop software. 
- At its core, Hadoop is a Java–based MapReduce framework.  
- Hadoop remained a system in which users submitted jobs that ran on the entire cluster.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Hadoop</span>
-  Jobs would be executed in a First In, First Out (FIFO) mode. 
- However, this lead to situations in which a long-running, less important job would hog resources and not allow a smaller yet more important job to execute. 
- To solve this problem, more complex job schedulers in Hadoop, such as the Fair Scheduler and Capacity Scheduler were created. 
- But Hadoop still had scalability limitations that were a result of some deeply entrenched design decisions resulting in Hadoop 2.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Hadoop Hive </span>(1/2)
- Hadoop Users realized that developing MapReduce programs is a very programming-intensive task, which makes it errorprone and hard to test. 
- There was a need for more expressive languages such as SQL 
- Required to enable users to focus on the problem instead of low-level implementations of typical SQL artifacts 
- Apache Hive evolved to provide a data warehouse (DW) capability to large datasets. 
- Users can express their queries in Hive Query Language, which is very similar to SQL. 
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Hadoop Hive </span>(1/2)
- The Hive engine converts these queries to low-level MapReduce jobs transparently. 
- More advanced users can develop user-defined functions (UDFs) in Java. 
- Hive also supports standard drivers such as ODBC and JDBC. 
- Hive is also an appropriate platform to use when developing Business Intelligence (BI) types of applications for data stored in Hadoop.
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Hadoop Pig </span> 
- Motivation for Pig was similar to Hive 
- Hive is a SQL-like language, which is declarative. 
- Pig is a procedural language that works well in data pipeline scenarios. 
- Pig appeals to programmers who develop data-processing pipelines  
- It is also an appropriate platform to use for extract, load, and transform (ELT) types of applications.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436">Hadoop Hbase</span>
- All the preceding projects, including MapReduce, are batch processes. 
- There is a strong need for real–time data lookup in Hadoop. 
- Hadoop did not have a native key/value store. 
- Consider a Social Media site such as Facebook. 
- If you want to look up a friend’s profile, you expect to get an answer immediately (not after a long batch job runs).

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Hadoop</span>
- Hadoop is a distributed data store. 

- Reliability of this data store, coupled with its flexibility in running multiple processing frameworks makes it an ideal choice for your data hub. 

- This characteristic of Hadoop means that you can store any type of data as is, without placing any constraints on how that data is processed.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436"> Hadoop</span>

- Schema-on-Read. This simply refers to the fact that raw, unprocessed data can be loaded into Hadoop, with the structure imposed at processing time based on the requirements of the processing application.

- Schema-on-Write, which is generally used with traditional data management systems. Such systems require the schema of the data store to be defined before the data can be loaded. This leads to lengthy cycles of analysis, data modeling, data transformation, loading, testing, and so on before data can be accessed.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436">HDFS</span>
- HDFS is Hadoop distributed file system. 
- Designed to run on commodity hardware. 
- Built to support high throughput, streaming reads and writes of huge files. 
- Highly fault-tolerant 
- Designed to be deployed on low-cost hardware. 

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436">HDFS</span>
- Offers centralized, low-latency access to large file systems. 
- It has many similarities with existing distributed file systems. 
- Relaxes a few POSIX requirements 
- Enables streaming access to file system data. 
- Was originally built as infrastructure for the Apache Nutch web search engine project. 
- It is now an Apache Hadoop subproject. 
---
<!-- .slide: data-autoslide="2000" -->
###  <span style="color: #e49436">Goals </span>
- Large Data Sets 
+	Applications that run on HDFS have large data sets. 
+	A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. 
+	It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. 
+	It should support tens of millions of files in a single instance. 

---
<!-- .slide: data-autoslide="2000" -->
###  <span style="color: #e49436">Goals</span>
- Hardware Failure 
+ Hardware failure is the norm rather than the exception. 
+ HDFS instance may consist of hundreds or thousands of server machines.
+ Huge number of components interact 
---
<!-- .slide: data-autoslide="2000" -->
###  <span style="color: #e49436">Goals</span>
- Hardware Failure 
+ Each component has a non-trivial probability of failure 
+ Some component of HDFS is always non-functional. 
+ Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS. 

---
<!-- .slide: data-autoslide="2000" -->
###  <span style="color: #e49436">Goals</span>
- Streaming Data Access 
    +  Applications on HDFS need streaming access to data sets. 
    +  HDFS is designed more for batch processing rather than interactive use by users. 
    +  Emphasis is on high throughput of data access rather than low latency of data access. 
     + POSIX imposes hard requirements that are targeted for HDFS. 
      + POSIX semantics in a few key areas has been traded to increase data throughput rates. 
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436">HDFS Similarities</span>
- HDFS has many similarities to a traditional file system. The following are some of them:
   + Files are stored in blocks. 
   + Metadata is available, which keeps track of filenames to block mapping.
   + It also supports the directory tree structure, as a traditional file system.
   + It works on the permission model. 
   + You can give different access rights on the file to different users.

---
<!-- .slide: data-autoslide="2000" -->
###  <span style="color: #e49436">HDFS Differences</span>
-  Very low storage cost per byte: 
   + HDFS uses commodity storage 
   + Shares the cost of the network it runs on with other systems of the Hadoop stack. 
   + Reduces the total cost of ownership. 
   + Allows an organization to store the same amount of data at a very low cost compared to traditional NAS or SAN systems.

---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436">HDFS Differences</span>
- Block size for the data: 
   + Traditional file systems generally use around 8 KB block size for the data, 
   + HDFS uses larger block size of data. 
    + Block size in HDFS is 128 MB, 
    + Admin can raise it to 1 GB or higher. 
---
<!-- .slide: data-autoslide="2000" -->
###  <span style="color: #e49436">HDFS Differences</span>
- Larger block size ensures that the data can be read and written in large sequential operations. 
- It can improve performance 
- it minimizes drive seek operations, especially when performing large I/O streaming operations.
---
<!-- .slide: data-autoslide="2000" -->
### <span style="color: #e49436">HDFS Differences</span>
-  Data protection mechanisms: 

   + Traditional file systems use specialized data storage for data protection. 
   + HDFS replicates each block to multiple machines in the cluster. 
   + By default, it replicates the data block to three nodes. 
   + Ensures data reliability and high availability

---
<!-- .slide: data-autoslide="2000" -->
### What is <span style="color: #e49436">Name Node</span>
- Is a centralized service in the cluster operating on a single node. 
- Manages the file system namespace 
- This is file system tree and the metadata for all the files and directories are maintained. 
- It is the arbitrator and repository for all HDFS metadata. 
- Is designed in such a way that user data never flows through the Name Node. 

---
<!-- .slide: data-autoslide="2000" -->
### What is <span style="color: #e49436">Name Node</span>
- Maintains and stores the namespace tree and the mapping of file blocks to Data Nodes 
- Two files are used
  + the namespace image
  + the edit log
---
<!-- .slide: data-autoslide="2000" -->
### What is <span style="color: #e49436">Name Node</span>
- File system metadata is stored on a metadata server. 
- Metadata operations may be handled by a single metadata server
- Cluster will configure multiple metadata servers as primary-backup failover pairs. 
- Includes the namespace, data location and access permissions. 
- Clients contact the Name Node in order to perform common file system operations, such as open, close, rename, and delete. 

---
<!-- .slide: data-autoslide="2000" -->
### What is <span style="color: #e49436">Heart Beat</span>
- Name Node does not store HDFS data 
- Maintains a mapping between HDFS file name, a list of blocks in the file, and the Data Node(s) on which those blocks are stored. 
- Periodically receives a Heartbeat and a Block report from each of the Data Nodes in the cluster. 
- Receipt of a Heartbeat implies that the Data Node is functioning properly. 
- Block report contains a list of all blocks on a Data Node.

---
<!-- .slide: data-autoslide="2000" -->
### Unstructured <span style="color: #e49436">Data Node</span>
- Daemon responsible for storing and retrieving block data is called the datanode (DN). 
- Responsible for serving read and write requests from clients 
- Perform block operations upon instructions from name node. 
---
<!-- .slide: data-autoslide="2000" -->
### Unstructured <span style="color: #e49436">Data Node</span>
- Stores HDFS blocks on behalf of local or remote clients. 
- Block is saved as a separate file in the node’s local file system. 
- Data Node abstracts away details of the local storage arrangement, 
---
<!-- .slide: data-autoslide="2000" -->
### Unstructured <span style="color: #e49436">Data Node</span>
- Nodes do not have to use the same local file system. 
- Blocks are created or destroyed on Data Nodes at the request of the Name Node, which validates and processes requests from clients. 
- Clients communicate directly with Data Nodes in order to read or write data at the HDFS block level. 
---
<!-- .slide: data-autoslide="2000" -->
### Unstructured <span style="color: #e49436">Data Node</span>
- Data node normally has no knowledge about HDFS files. 
- Scans through the local file system 
- Creates a list of HDFS data blocks corresponding to each of these local files 
- Sends this report to the Name node. 
---
<!-- .slide: data-autoslide="2000" -->
### Unstructured <span style="color: #e49436">Data Node</span>
- Individual files are broken into blocks of a fixed size 
- Distributed across multiple DataNodes in the cluster. 
- The Name Node maintains metadata about the size and location of blocks and their replicas
---
<!-- .slide: data-autoslide="2000" -->
### Structured <span style="color: #e49436">Client </span>
- Client is an api of applications. 
- Communicates with the Namenode for metadata 
- Once received it directly runs operations on the Datanodes. 
- If the operation is a MapReduce operation, the client creates a job and sends it to the queue. 

---
<!-- .slide: data-autoslide="2000" -->
### Structured <span style="color: #e49436">Client </span>
- JobTracker handles this queue. 
- Client perform file metadata operations such as create file and open file, at the NameNode over an RPC protocol and read/write the data of a file directly to DataNodes using a streaming socket protocol called the data-transfer protocol.

---
<!-- .slide: data-autoslide="2000" -->
### External <span style="color: #e49436">Blocks</span>
- Disk has a block size, 
- is the minimum amount of data that it can read or write. 
- File systems for a single disk build by dealing with data in blocks, 
- An integral multiple of the disk block size. 

---
<!-- .slide: data-autoslide="2000" -->
### External <span style="color: #e49436">Blocks</span>
- File system blocks are typically a few kilobytes in size, 
- Disk blocks are normally 512 bytes. 
- Transparent to the file system user who is simply reading or writing a file of whatever length. 
- There are tools to perform file system maintenance, such as df and fsck, that operate on the file system block level.

---
<!-- .slide: data-autoslide="2000" -->
### Internal <span style="color: #e49436">Large blocks</span>
- HDFS, has the concept of a block
- Much larger unit 128 MB. 
- Files in HDFS are broken into block-sized chunks, 
- Stored as independent units.

---
<!-- .slide: data-autoslide="2000" -->
### Internal <span style="color: #e49436">Large blocks</span>
- Are large compared to disk blocks, the reason is to minimize the cost of seeks. 
- By making a block large enough, the time to transfer the data from the disk can be significantly longer than the time to seek to the start of the block. 
- The time to transfer a large file made of multiple blocks operates at the disk transfer rate. 

---
<!-- .slide: data-autoslide="2000" -->
### Internal <span style="color: #e49436">Large blocks</span>
- Having a block abstraction for a distributed filesystem brings several benefits. 
- File can be larger than any single disk in the network. 
- Take advantage of any of the disks in the cluster. 
- In fact, it would be possible, if unusual, to store a single file on an HDFS cluster whose blocks filled all the disks in the cluster

---
<!-- .slide: data-autoslide="15000" -->
### What is <span style="color: #e49436">Large Blocks </span>
- Making the unit of abstraction a block rather than a file simplifies the storage subsystem. 
- Storage subsystem deals with blocks, simplifying storage management and eliminating metadata concerns 
- Blocks are just chunks of data to be stored, 
- file metadata such as permissions information does not need to be stored with the blocks, so another system can handle metadata separately
---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436">Large blocks </span>
-  Blocks fit well with replication for providing fault tolerance and availability. 
- Insure against corrupted blocks and disk and machine failure, 
---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436">Large blocks </span>
- Block is replicated to a small number of physically separate machines.
- Block becomes unavailable, a copy can be read from another location in a way that is transparent to the client.
- Block that is no longer available due to corruption or machine failure can be replicated from its alternative locations 
---
<!-- .slide: data-autoslide="15000" -->
###  <span style="color: #e49436"> File system Namespace</span>
- Traditional local file systems support a persistent name space. 
- Local file system views devices as being locally attached, 
- The devices are not shared, and hence there is no need in the file system design to enforce device sharing semantics.
---
<!-- .slide: data-autoslide="15000" -->
###  <span style="color: #e49436"> File system Namespace</span>
- HDFS supports a traditional hierarchical file organization. 
- User or an application can create directories and store files inside these directories. 
- File system namespace hierarchy is similar to most other existing file systems 
---
<!-- .slide: data-autoslide="15000" -->
###  <span style="color: #e49436"> File system Namespace</span>
- Create and remove files, move a file from one directory to another, or rename a file. 
- HDFS does not yet implement user quotas or access permissions. 
- HDFS does not support hard links or soft links. 
- HDFS architecture does not preclude implementing these features
---
<!-- .slide: data-autoslide="15000" -->
###  <span style="color: #e49436"> Federation </span>
- HDFS Federation, introduced in the 2.x release series, 
- Allows a cluster to scale by adding name nodes, 
- Manages a portion of the file system namespace.

---
<!-- .slide: data-autoslide="15000" -->
###  <span style="color: #e49436"> Federation </span>
- Each name node manages a namespace volume
- Made up of the metadata for the namespace 
- Made up of a block pool containing all the blocks for the files in the namespace. 

---
<!-- .slide: data-autoslide="15000" -->
###  <span style="color: #e49436"> Federation </span>
- Namespace volumes are independent of each other 
- Name nodes do not communicate with one another 
- Failure of one name node does not affect the availability of the namespaces managed by other name nodes. 

---
<!-- .slide: data-autoslide="15000" -->
###  <span style="color: #e49436"> Federation </span>
- Block pool storage is not partitioned 
- Data nodes register with each name node in the cluster and store blocks from multiple block pools. 
- To access a federated HDFS cluster, clients use client-side mount tables to map file paths to name nodes. 
- This is managed in configuration using ViewFileSystem and the viewfs:// URIs.

---
<!-- .slide: data-autoslide="15000" -->
### Function of <span style="color: #e49436"> Configuration Files </span>
- Every machine in the Hadoop Cluster has its own set of configuration files. 
- Earlier  Hadoop had a single configuration file: hadoop-site.xml. 
- Subsequent versions of Hadoop split this file into multiple files based on functionality. 
- Additionally, there are two types of configuration files: 
   +  *-default.xml  
   +  *-site.xml. 

---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436">conf Files</span>
-  *-site.xml file configurations override the ones in the *-default.xml file. The *-default.xml files are read-only and are read directly from the JAR files in the classpath.
   +  core-default.xml Default core Hadoop properties. 
   + The file is located in the following JAR file: hadoop-common-2.2.0.jar (assuming version 2.2.0).

---
<!-- .slide: data-autoslide="15000" -->
###  <span style="color: #e49436">conf Files</span>
   +  hdfs-default.xml Default HDFS properties. 
   + The file is located in the following JAR file: hadoop-hdfs-2.2.0.jar (assuming version 2.2.0).

---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436">conf Files</span>
   + mapred-default.xml Default MapReduce properties. 
   + The file is located in the following JAR file: hadoop-mapreduce-client-core-2.2.0.jar (assuming version 2.2.0).

---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436">conf Files</span>
   +  yarn-default.xml Default YARN properties. 
   + The file is located in the following JAR file: hadoop-yarn-common-2.2.0.jar (assuming version 2.2.0).

---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436"> Installation Procedure</span>

- Required software for Linux include:
+  Java™ must be installed.
+  ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons.
---
<!-- .slide: data-autoslide="15000" -->
### Virus <span style="color: #e49436"> Core-site.xml  </span>
 
 <configuration>
     <property>
     <name>fs.defaultFS</name>
     <value>hdfs://MastercomputerIP or localhost:9000</value>
     </property>
  </configuration>


---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436"> Mutation engine</span>
- Heart of a metamorphic virus is a mutation engine,
- Responsible for transforming its program. 
- takes an input program and morphs it to a structurally different but semantically equivalent program
- Three modules of any mutation engine: 
  + disassembly module, reverse engineering module, transformation module
  
---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Worm</span>

- Computer worm is a standalone malware computer program
- Replicates itself in order to spread to other computers
- Worms spread by exploiting vulnerabilities in operating systems.
- Worm searches a vulnerable host to replicate itself 

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Worm</span> Types

-  *Email-Worms*: Spreads via email messages.
-  *Instant-Messaging* Worms: Spread useing instant messaging applications
-  *File-sharing* worms copies itself into a shared folder
-  *Internet-worms* target low level TCP/IP ports directly

---
<!-- .slide: data-autoslide="15000" -->

### Difference <span style="color: #e49436">Worm - virus?</span>

- Both are malware and can perform the same malicious actions.  
- Viruses typically don’t self-propagate, and rely on users  to activate and transport the virus to a new destination.  
- Worms are generally self-propagating, 
 
---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436"> Trojan </span>

- Trojan Horse is a destructive program that has been disguised
- Looks like an innocuous software.
- Worms / virus programs concealed in a Trojan Horse.
- Trojan Horses are not viruses
- They do not reproduce themselves and spread as viruses do


---
<!-- .slide: data-autoslide="15000" -->

### Types of <span style="color: #e49436"> Trojan</span>

Joke Trojans
NVP Trojan
IconDance Trojan
Destructive Trojans

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436"> Joke Trojans </span>
- Joke Trojan causes no damage 
- Play an annoying sound from your computer's speaker, 
- Warp the appearance or display a taunting message on the screen, 
- Msg is "Now formatting hard drive!" 
- are harmless and easily deleted.

---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436"> NVP Trojan</span>

- Is a Macintosh Trojan horse 
- Modifies the system file  
- User types any text, the vowels (a, e, i, o, and u) fail to appear. 
- Trojan masquerades as a utility program 

---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436"> IconDance Trojan</span>

- IconDance Trojan minimizes all application windows 
- Starts rapidly scrambling all the desktop icons. 
- It does nothing more than make you take the time to reorganize your Windows desktop.

---
<!-- .slide: data-autoslide="15000" -->
### <span style="color: #e49436"> Destructive Trojan </span>

- Can either wipe out your hard drive or selectively delete or modify certain files. 
- Although these are the most dangerous Trojans, their very nature tends to limit their spread: In the process of attacking your computer, they reveal their presence, often by displaying a taunting message on the screen. And, if they reformat your hard drive, they also wipe themselves out.


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436"> Spyware</span>

- Spyware is a type of malware
- Aims to gather information about a person or organization
- Info gathered without their knowledge,
- Information sent to another entity without the consumer’s consent,
- Variety of Spyware in circulation


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Spyware-Types </span>(1/2)

- Browser Hijack: Control web browser and display unsolicited advertisements.
- Adware: Monitors online activities for targeted Pop-Up advertising.
- Trojans: Make your computer vulnerable ti remote controll by hackers.
- Profiling Cookies: Track your web surfing habits.
- Keyboard Loggers and Trackware Programs: Capture and record your every keystroke, inlcuding personal information and passwords.


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Spyware-Types.. </span> (2/2)

- Dialers Auto-dial toll calls without your permission.
- Droneware Control your PC, to send spam or to host offense web images.
- Web Bugs: A graphics designed to monitor the reader. Often invisible because they are typically only 1-by-1 pixel in size. They are represented as HTML IMG tags.


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436"> Grayware </span>

- Browser hijacker - software that modifies the default browser behavior.
- Homepage hijacker - modifies the default home page
- Search hijacker modifies the default search engine of the browser.
- Error page hijacker causes the browser to display a particular web site whenever a misspelled URL is entered into the address bar.



---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Zeus Trojan </span>

- First spotted in 2007, 
- specifically designed to create a botnet of compromised computers 
- To be recruited for information stealing and financial fraud activities. 
- Spreads through phishing and drive-by attacks 
- collects information using various techniques like form grabbing and key-stroke monitoring.  

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Zeus Trojan </span>

- Estimate Zeus botnet had infected more than 3.6  million computers
- toolkit comes with an easy installation procedure and customization mechanisms 
- Zeus  uses  complicated  multilevel  obfuscation  mechanisms  
- this is to avoid  detection and hinder any analysis procedure. 


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Stuxnet worm</span>
- Designed to specifically target SCADA systems   
- Employ  PLC  rootkit technology that  allowed  PLC code  modifications. 
- initially  found  to  target  Iranian  facilities.
- quickly  spread  to various other countries. 
- position as  one  of  the  most dangerous cyber weapons.

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Stuxnet worm</span>
- Designed to propagate through various zero day vulnerabilities,   
- Unknown  exploits on  print spooler service, Microsoft Windows Server services used  
- features include 
  + stolen component certificates, 
  + complicated injection and hooking mechanisms, 
  + dynamic updates  and specific antivirus evasion utilities. 

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Dugu worm</span>

- 2011 a  new  sophisticated  worm  attack  was  identified.  
- Dugu was specifically designed  to  
  + steal  passwords,  
  + collect computer screenshots located on the infected machine. 
- Dugu spreads through a previously unknown vulnerability in MS word documents
- main purpose is to conduct industrial espionage.

---
<!-- .slide: data-autoslide="2000" -->

### <span style="color: #e49436">Recap</span>

---
<!-- .slide: data-autoslide="2000" -->

## Agenda
- Botnets
  + Botnets - types of botnet- Structure of bots – 
  + Crime bots - Spamming bots - 
  + DoS – DDoS Attacks – types - Honey Pots
---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">BotNet</span>

- Term “botnet” coined from “robot networks.” 
- Robot” has a Czech derivation from the word “robotovat,” which means “to work.” 
- Botnet, therefore, is an apt definition: 
- bots are highly adaptable worker bees that do their master’s bidding

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">BotNet</span>
- The overall architecture and implementation of botnets is complex
- Evolving toward the use of common software engineering techniques such as modularity
- The predominant remote control mechanism for botnets remains Internet Relay Chat (IRC) 
- IRC includes a rich set of commands enabling a wide range of use

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">BotNet</span>

- Wide diversity of exploits for infecting target systems written into botnet codebases.
- All botnets include denial of service (DoS) attack capability.
- Shell encoding and packing mechanisms that can enable attacks to circumvent defensive systems are common
- All botnets include a variety of sophisticated mechanisms for avoiding detection

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">BotNet</span>

- New botnets use different organizational techniques, including:
- Decentralized Naming Resolution. 
  + Using techniques pioneered by spammers botnets now act as their own DNS cloud, 
  + avoid centralized name resolution. 
  + Victims are instructed to use existing botnets for DNS resolution, avoiding the centralized use of naming services.

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">BotNet</span>

- Tor botnets. 
  + IRC operators have reported botnets on the Undernet network connecting from TOR exit nodes. 
  + Tor is a proxy network 
  + Uses a type of MIX-net routing to anonymize traffic. 
  + Botnets using Tor therefore pose difficult detection and response problems

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">BotNet</span>

- Tunneling bots. 
  + Increase in the number of bots that tunnel through existing protocols.
  + Bots potentially use net news, web blogs, and other resources. 
  + Fundamentally, these sorts of bots pose more of a problem for detection.


---
<!-- .slide: data-autoslide="2000" -->

### <span style="color: #e49436">Recap</span>
---

<!-- .slide: data-autoslide="2000" -->

## Agenda
- Network forensics  
  + Key Loggers - Port Scans – SYN flood - 
  + Protocols Susceptible to Sniffing - Active and Passive Sniffing- 
  + Wireshark – Capture and Display Filters - pcap analysis – Problems -  
  + Forensic evidences - log analysis & evidence collection. 


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Key Loggers</span>


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Key Loggers</span>


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Port Scan</span>



---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">SYN flood</span>


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Sniffing</span>
- Sniffing is the electronic form of eavesdropping 
- Carried out on communications, computers transmit across networks
- Sniffing involves 
  + capturing, decoding, inspecting and interpreting 
  + information inside a network packet on a TCP/IP network. 
- Acts as network probes for examining network traffic.


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Packer Sniffer</span>

- Packet sniffer is a utility 
- used since the original release of Ethernet. 
- Allows individuals to capture data as it is transmitted over a network 
- Used by network professionals to diagnose network issues, 
- Malicious capture unencrypted data, like passwords and usernames.

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Active Sniffing</span>

- Sniffing is performed on a switched network, known as active sniffing.
- Relies on injecting packets into the network that causes traffic. 
- Is required to bypass the segmentation that switches provided. 
- Sniffers operate at the Data Link layer of the OSI model.  
- Grab whatever they see on the wire and record it for later review. 

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Passive Sniffing</span>

- Hubs see all traffic in the collision domain. 
- Sniffing performed on a hub is known as passive sniffing.
- Collision domain is a logical area of the network in which one or more data packets can collide with each other.
- Passive sniffing worked well during the days that hubs were used. 

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Protocols - Sniffing</span>
- Telnet and Re-login: 
- HTTP: Passwords and data sent 
- SNMP: SNMPv1 no good security. SNMP passwords sent in clear text.
- NNTP: Passwords and data sent.
- POP, IMAP: Passwords and data sent.
- FTP: Passwords and data sent.

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Sniffing Tools</span>

- Wireshark.
- Tcpdump.
- Dsniff.
- NetStumbler.
- Ettercap.
- Ntop.

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Wireshark</span>
- open source  tool with GUI
- has more filtering  and sorting options,
- first window shows packet capture, second packet detail and last raw data 
- command line version called  Tshark.  
- Supports more  than  1100 protocols. 

---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">Wireshark</span>

- Supports  geolocalisation  of  MaxMind 
- Cities and localities could be seen by given IPs giving the information of origin of packets. 
- not  for  layman as it involves a lot of network layer filtering options.


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">TCPDump</span>

- Used for packet capturing, network  monitoring  and  protocol  debugging.
- Oldest and most commonly used command line tool, 
- Free  and  open  source software.  
- used  to  read  live  capture  or already captured log file
- least overhead since no graphical  interface 
- Captures data  in  libpcap  formats, 


---
<!-- .slide: data-autoslide="15000" -->

### <span style="color: #e49436">PCAP Analysis</span>


---




![Thanks](images/thanks.png)



